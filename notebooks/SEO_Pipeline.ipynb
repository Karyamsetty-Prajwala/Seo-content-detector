{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5ab41-0a41-4531-82cf-d1b3ad486872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP1-Data Collection & HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "1d4e0462-7b52-49ed-8d00-19932fb96803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>html_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;!--[if lt IE 7]&gt; &lt;html class=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\\n    &lt;me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html data-unhead-vue-server-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>\\n\\n&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                        html_content  \n",
       "0  <!doctype html><!--[if lt IE 7]> <html class=\"...  \n",
       "1  <!doctype html><html lang=\"en\"><head>\\n    <me...  \n",
       "2  <!DOCTYPE html><html data-unhead-vue-server-re...  \n",
       "3  \\n\\n<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\"...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('seo-content-detector/data/data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8f322d94-aae1-46fa-93d6-f3840c2f9cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 0: 'htmlcontent'\n",
      "Error processing row 1: 'htmlcontent'\n",
      "Error processing row 2: 'htmlcontent'\n",
      "Error processing row 3: 'htmlcontent'\n",
      "Error processing row 4: 'htmlcontent'\n",
      "Error processing row 5: 'htmlcontent'\n",
      "Error processing row 6: 'htmlcontent'\n",
      "Error processing row 7: 'htmlcontent'\n",
      "Error processing row 8: 'htmlcontent'\n",
      "Error processing row 9: 'htmlcontent'\n",
      "Error processing row 10: 'htmlcontent'\n",
      "Error processing row 11: 'htmlcontent'\n",
      "Error processing row 12: 'htmlcontent'\n",
      "Error processing row 13: 'htmlcontent'\n",
      "Error processing row 14: 'htmlcontent'\n",
      "Error processing row 15: 'htmlcontent'\n",
      "Error processing row 16: 'htmlcontent'\n",
      "Error processing row 17: 'htmlcontent'\n",
      "Error processing row 18: 'htmlcontent'\n",
      "Error processing row 19: 'htmlcontent'\n",
      "Error processing row 20: 'htmlcontent'\n",
      "Error processing row 21: 'htmlcontent'\n",
      "Error processing row 22: 'htmlcontent'\n",
      "Error processing row 23: 'htmlcontent'\n",
      "Error processing row 24: 'htmlcontent'\n",
      "Error processing row 25: 'htmlcontent'\n",
      "Error processing row 26: 'htmlcontent'\n",
      "Error processing row 27: 'htmlcontent'\n",
      "Error processing row 28: 'htmlcontent'\n",
      "Error processing row 29: 'htmlcontent'\n",
      "Error processing row 30: 'htmlcontent'\n",
      "Error processing row 31: 'htmlcontent'\n",
      "Error processing row 32: 'htmlcontent'\n",
      "Error processing row 33: 'htmlcontent'\n",
      "Error processing row 34: 'htmlcontent'\n",
      "Error processing row 35: 'htmlcontent'\n",
      "Error processing row 36: 'htmlcontent'\n",
      "Error processing row 37: 'htmlcontent'\n",
      "Error processing row 38: 'htmlcontent'\n",
      "Error processing row 39: 'htmlcontent'\n",
      "Error processing row 40: 'htmlcontent'\n",
      "Error processing row 41: 'htmlcontent'\n",
      "Error processing row 42: 'htmlcontent'\n",
      "Error processing row 43: 'htmlcontent'\n",
      "Error processing row 44: 'htmlcontent'\n",
      "Error processing row 45: 'htmlcontent'\n",
      "Error processing row 46: 'htmlcontent'\n",
      "Error processing row 47: 'htmlcontent'\n",
      "Error processing row 48: 'htmlcontent'\n",
      "Error processing row 49: 'htmlcontent'\n",
      "Error processing row 50: 'htmlcontent'\n",
      "Error processing row 51: 'htmlcontent'\n",
      "Error processing row 52: 'htmlcontent'\n",
      "Error processing row 53: 'htmlcontent'\n",
      "Error processing row 54: 'htmlcontent'\n",
      "Error processing row 55: 'htmlcontent'\n",
      "Error processing row 56: 'htmlcontent'\n",
      "Error processing row 57: 'htmlcontent'\n",
      "Error processing row 58: 'htmlcontent'\n",
      "Error processing row 59: 'htmlcontent'\n",
      "Error processing row 60: 'htmlcontent'\n",
      "Error processing row 61: 'htmlcontent'\n",
      "Error processing row 62: 'htmlcontent'\n",
      "Error processing row 63: 'htmlcontent'\n",
      "Error processing row 64: 'htmlcontent'\n",
      "Error processing row 65: 'htmlcontent'\n",
      "Error processing row 66: 'htmlcontent'\n",
      "Error processing row 67: 'htmlcontent'\n",
      "Error processing row 68: 'htmlcontent'\n",
      "Error processing row 69: 'htmlcontent'\n",
      "Error processing row 70: 'htmlcontent'\n",
      "Error processing row 71: 'htmlcontent'\n",
      "Error processing row 72: 'htmlcontent'\n",
      "Error processing row 73: 'htmlcontent'\n",
      "Error processing row 74: 'htmlcontent'\n",
      "Error processing row 75: 'htmlcontent'\n",
      "Error processing row 76: 'htmlcontent'\n",
      "Error processing row 77: 'htmlcontent'\n",
      "Error processing row 78: 'htmlcontent'\n",
      "Error processing row 79: 'htmlcontent'\n",
      "Error processing row 80: 'htmlcontent'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_features(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.string if soup.title else \"\"\n",
    "    # Extract body text from common content tags\n",
    "    body_elements = soup.find_all(['article', 'main', 'p'])\n",
    "    body_text = \" \".join([elem.get_text(separator=\" \", strip=True) for elem in body_elements])\n",
    "    if not body_text:\n",
    "        body_text = soup.get_text(separator=\" \", strip=True)\n",
    "    wordcount = len(body_text.split())\n",
    "    return title, body_text, wordcount\n",
    "\n",
    "features = []\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        title, body_text, wordcount = extract_text_features(row['htmlcontent'])\n",
    "        features.append({'url': row['url'], 'title': title, 'bodytext': body_text, 'wordcount': wordcount})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv('data/extractedcontent.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "65763e06-3ffd-4bfc-99f5-b967fae3f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', 'html_content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/data.csv')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a8b827ee-c4c9-433b-a17e-4b031c4176d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 4: object of type 'float' has no len()\n",
      "Error processing row 12: object of type 'float' has no len()\n",
      "Error processing row 18: object of type 'float' has no len()\n",
      "Error processing row 20: object of type 'float' has no len()\n",
      "Error processing row 23: object of type 'float' has no len()\n",
      "Error processing row 40: object of type 'float' has no len()\n",
      "Error processing row 41: object of type 'float' has no len()\n",
      "Error processing row 51: object of type 'float' has no len()\n",
      "Error processing row 55: object of type 'float' has no len()\n",
      "Error processing row 75: object of type 'float' has no len()\n",
      "Error processing row 76: object of type 'float' has no len()\n",
      "Error processing row 77: object of type 'float' has no len()\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_features(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.string if soup.title else \"\"\n",
    "    body_elements = soup.find_all(['article', 'main', 'p'])\n",
    "    body_text = \" \".join([elem.get_text(separator=\" \", strip=True) for elem in body_elements])\n",
    "    if not body_text:\n",
    "        body_text = soup.get_text(separator=\" \", strip=True)\n",
    "    wordcount = len(body_text.split())\n",
    "    return title, body_text, wordcount\n",
    "\n",
    "features = []\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        title, body_text, wordcount = extract_text_features(row['html_content'])\n",
    "        features.append({'url': row['url'], 'title': title, 'bodytext': body_text, 'wordcount': wordcount})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv('extractedcontent.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "da29cb06-f67b-4447-aca0-8e27bba642ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html_content\n",
      "True     69\n",
      "False    12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df['html_content'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "80709964-a7f3-42a3-8579-4b23058113bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows processed: 69\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_features(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.string if soup.title else \"\"\n",
    "    body_elements = soup.find_all(['article', 'main', 'p'])\n",
    "    body_text = \" \".join([elem.get_text(separator=\" \", strip=True) for elem in body_elements])\n",
    "    if not body_text:\n",
    "        body_text = soup.get_text(separator=\" \", strip=True)\n",
    "    wordcount = len(body_text.split())\n",
    "    return title, body_text, wordcount\n",
    "\n",
    "features = []\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        html = row['html_content']\n",
    "        # Skip if not a string or is empty\n",
    "        if not isinstance(html, str) or not html.strip():\n",
    "            continue\n",
    "        title, body_text, wordcount = extract_text_features(html)\n",
    "        features.append({'url': row['url'], 'title': title, 'bodytext': body_text, 'wordcount': wordcount})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "# Save output\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv('extractedcontent.csv', index=False)\n",
    "print(f\"Number of rows processed: {len(features)}\")\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv('extractedcontent.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "71e37abe-e8a7-4bd8-a0bb-e5e8872f7fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kpraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kpraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://nordlayer.com/learn/network-security/b...   \n",
      "\n",
      "                                      clean_bodytext  \n",
      "0  cyber crisi tabletop exercis cyber secur awar ...  \n",
      "1  data secur platform buy capabl data discoveri ...  \n",
      "2  home insight blog post cyber defens tip stay s...  \n",
      "3  offici websit unit state govern know offici we...  \n",
      "4  cut edg busi vpn holist approach secur connect...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load extracted content\n",
    "df = pd.read_csv('extractedcontent.csv')\n",
    "\n",
    "# Download NLTK data if needed\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags if any\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove non-alphanumeric characters\n",
    "    words = word_tokenize(text)\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['clean_bodytext'] = df['bodytext'].astype(str).apply(preprocess_text)\n",
    "df.to_csv('preprocessed_content.csv', index=False)\n",
    "print(df[['url', 'clean_bodytext']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2b3c8-34e3-4cce-8a0c-e946a2aca9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP2-Text Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "121e8154-7ae1-4d8f-8d60-6e099edf2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  title_wordcount  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog                3   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips                7   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...                7   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...                8   \n",
      "4  https://nordlayer.com/learn/network-security/b...                5   \n",
      "\n",
      "   body_wordcount  \n",
      "0             234  \n",
      "1            3284  \n",
      "2            1098  \n",
      "3             989  \n",
      "4            2756  \n"
     ]
    }
   ],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def title_length(text):\n",
    "    return len(text.split())\n",
    "\n",
    "df['clean_title'] = df['title'].astype(str).apply(preprocess_text)\n",
    "df['title_wordcount'] = df['clean_title'].apply(count_words)\n",
    "df['body_wordcount'] = df['clean_bodytext'].apply(count_words)\n",
    "\n",
    "# Save augmented dataset\n",
    "df.to_csv('features_engineered.csv', index=False)\n",
    "print(df[['url', 'title_wordcount', 'body_wordcount']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "66021714-b85f-40e6-92f0-29fdacda652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents similar to the first URL (https://www.cm-alliance.com/cybersecurity-blog):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('features_engineered.csv')\n",
    "\n",
    "# Vectorize the clean body text using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_bodytext'])\n",
    "\n",
    "# Calculate pairwise cosine similarity matrix\n",
    "cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# To demonstrate, find duplicates for the first document with similarity > 0.8\n",
    "threshold = 0.8\n",
    "first_doc_sim = cos_sim_matrix[0]\n",
    "duplicates_idx = np.where(first_doc_sim > threshold)[0]\n",
    "\n",
    "print(f\"Documents similar to the first URL ({df.loc[0, 'url']}):\")\n",
    "for idx in duplicates_idx:\n",
    "    if idx != 0:\n",
    "        print(f\" - {df.loc[idx, 'url']} with similarity {first_doc_sim[idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e496d514-e2e7-4fc5-845d-d96cbb2deef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  duplicate_cluster\n",
      "0      https://www.cm-alliance.com/cybersecurity-blog                 50\n",
      "1     https://www.varonis.com/blog/cybersecurity-tips                 29\n",
      "2   https://www.cisecurity.org/insights/blog/11-cy...                 30\n",
      "3   https://www.cisa.gov/topics/cybersecurity-best...                 51\n",
      "4   https://nordlayer.com/learn/network-security/b...                 24\n",
      "5   https://www.fortinet.com/resources/cyberglossa...                 25\n",
      "6   https://www.cisco.com/site/us/en/learn/topics/...                 27\n",
      "7   https://www.trendmicro.com/en_us/what-is/netwo...                 26\n",
      "8   https://digitdefence.com/blog/fundamentals-of-...                 28\n",
      "9   https://guardiandigital.com/resources/blog/gui...                 52\n",
      "10                           https://cofense.com/blog                 54\n",
      "11             https://www.phriendlyphishing.com/blog                 55\n",
      "12  https://inspiredelearning.com/blog/phishing-pr...                 53\n",
      "13               https://en.wikipedia.org/wiki/SD-WAN                 20\n",
      "14  https://www.cisco.com/site/us/en/learn/topics/...                 18\n",
      "15  https://www.fortinet.com/resources/cyberglossa...                 19\n",
      "16                  https://remotedesktop.google.com/                 56\n",
      "17  https://support.apple.com/guide/remote-desktop...                 58\n",
      "18  https://en.wikipedia.org/wiki/Remote_desktop_s...                 57\n",
      "19  https://www.fortinet.com/solutions/enterprise-...                 23\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import pandas as pd\n",
    "\n",
    "# Use precomputed cosine similarity matrix (cos_sim_matrix)\n",
    "# Convert similarity matrix to distance matrix (distance = 1 - similarity)\n",
    "distance_matrix = 1 - cos_sim_matrix\n",
    "\n",
    "# Perform hierarchical clustering using 'average' linkage\n",
    "Z = linkage(distance_matrix, method='average')\n",
    "\n",
    "# Create cluster labels with a threshold for maximum distance\n",
    "max_distance = 0.2  # Corresponds to similarity cutoff of 0.8\n",
    "clusters = fcluster(Z, t=max_distance, criterion='distance')\n",
    "\n",
    "# Add cluster labels to your dataframe\n",
    "df['duplicate_cluster'] = clusters\n",
    "\n",
    "# Example: Show URLs and their cluster IDs\n",
    "print(df[['url', 'duplicate_cluster']].head(20))\n",
    "\n",
    "# Save with clusters\n",
    "df.to_csv('features_with_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb000d-6177-46a6-92de-6b1646c04b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP3-Duplicate Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6af423c3-da93-46f1-a903-8130dc848c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (0.7.10)\n",
      "Requirement already satisfied: pyphen in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from textstat) (3.9.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from nltk->textstat) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from nltk->textstat) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from nltk->textstat) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from nltk->textstat) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from click->nltk->textstat) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "849e10d6-b88c-4202-8941-e7de6048b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2410d480-098e-44a6-9571-4ae1a58c213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (10.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kpraj\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "03e23959-bbfa-4cd4-963e-3233ebc3946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://nordlayer.com/learn/network-security/b...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.014283106547502242, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01458775...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load preprocessed content CSV (with clean body text)\n",
    "df = pd.read_csv('preprocessed_content.csv')\n",
    "\n",
    "# Generate TF-IDF matrix as embedding representation\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_bodytext'])\n",
    "\n",
    "# Store embeddings as lists\n",
    "df['embedding'] = list(tfidf_matrix.toarray())\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv('text_features_with_tfidf_embeddings.csv', index=False)\n",
    "\n",
    "print(df[['url', 'embedding']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "39275741-e38d-4d59-b18f-16a16b5d90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', 'title', 'bodytext', 'wordcount', 'clean_bodytext', 'embedding'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0fc65e8f-fcca-4a72-9ebb-aa43b30e3a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages analyzed: 69\n",
      "Duplicate pairs found: 0\n",
      "Thin content pages: 14 (20.3%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# Make sure the data folder exists\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Load your features CSV - update the filename if needed\n",
    "df = pd.read_csv('data/text_features_with_tfidf_embeddings.csv')\n",
    "\n",
    "# Convert embedding strings to numpy arrays, ignore rows where parsing fails (in case of ellipsis, etc.)\n",
    "def parse_embedding(x):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(x))\n",
    "    except:\n",
    "        return np.zeros(10)  # or appropriate size\n",
    "\n",
    "df['embedding'] = df['embedding'].apply(parse_embedding)\n",
    "embedding_matrix = np.vstack(df['embedding'].values)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cos_sim_matrix = cosine_similarity(embedding_matrix)\n",
    "\n",
    "# Set similarity threshold\n",
    "similarity_threshold = 0.80\n",
    "\n",
    "# Find duplicate pairs (upper triangle only)\n",
    "duplicate_pairs = []\n",
    "num_docs = len(df)\n",
    "for i in range(num_docs):\n",
    "    for j in range(i + 1, num_docs):\n",
    "        sim = cos_sim_matrix[i, j]\n",
    "        if sim > similarity_threshold:\n",
    "            duplicate_pairs.append((df.loc[i, 'url'], df.loc[j, 'url'], sim))\n",
    "\n",
    "duplicates_df = pd.DataFrame(duplicate_pairs, columns=['url1', 'url2', 'similarity'])\n",
    "\n",
    "# Thin content detection (word count < 500)\n",
    "df['is_thin'] = df['wordcount'] < 500\n",
    "\n",
    "# Report\n",
    "total_pages = num_docs\n",
    "num_duplicate_pairs = len(duplicate_pairs)\n",
    "num_thin_pages = df['is_thin'].sum()\n",
    "percent_thin = (num_thin_pages / total_pages) * 100\n",
    "\n",
    "print(f\"Total pages analyzed: {total_pages}\")\n",
    "print(f\"Duplicate pairs found: {num_duplicate_pairs}\")\n",
    "print(f\"Thin content pages: {num_thin_pages} ({percent_thin:.1f}%)\")\n",
    "\n",
    "# Save all outputs to the data folder\n",
    "duplicates_df.to_csv('data/duplicates.csv', index=False)\n",
    "df.to_csv('data/text_features_with_tfidf_embeddings_with_thin.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc255f-9b13-45e5-aa6a-8f0268e3cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-4: Content Quality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5671bc1b-38e8-4750-aaf3-ee3244edd02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: LogisticRegression — Accuracy: 0.762  Macro F1: 0.760\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.714     0.833     0.769         6\n",
      "         Low      1.000     0.600     0.750         5\n",
      "      Medium      0.727     0.800     0.762        10\n",
      "\n",
      "    accuracy                          0.762        21\n",
      "   macro avg      0.814     0.744     0.760        21\n",
      "weighted avg      0.788     0.762     0.761        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       5    0       1\n",
       "Low        0    3       2\n",
       "Medium     2    0       8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: SVC — Accuracy: 0.667  Macro F1: 0.638\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.571     0.667     0.615         6\n",
      "         Low      1.000     0.400     0.571         5\n",
      "      Medium      0.667     0.800     0.727        10\n",
      "\n",
      "    accuracy                          0.667        21\n",
      "   macro avg      0.746     0.622     0.638        21\n",
      "weighted avg      0.719     0.667     0.658        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       4    0       2\n",
       "Low        1    2       2\n",
       "Medium     2    0       8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: KNN — Accuracy: 0.476  Macro F1: 0.357\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.400     0.667     0.500         6\n",
      "         Low      0.000     0.000     0.000         5\n",
      "      Medium      0.545     0.600     0.571        10\n",
      "\n",
      "    accuracy                          0.476        21\n",
      "   macro avg      0.315     0.422     0.357        21\n",
      "weighted avg      0.374     0.476     0.415        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       4    0       2\n",
       "Low        2    0       3\n",
       "Medium     4    0       6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: GaussianNB — Accuracy: 0.429  Macro F1: 0.278\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.000     0.000     0.000         6\n",
      "         Low      0.143     0.200     0.167         5\n",
      "      Medium      0.571     0.800     0.667        10\n",
      "\n",
      "    accuracy                          0.429        21\n",
      "   macro avg      0.238     0.333     0.278        21\n",
      "weighted avg      0.306     0.429     0.357        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       0    4       2\n",
       "Low        0    1       4\n",
       "Medium     0    2       8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: RandomForest — Accuracy: 0.476  Macro F1: 0.440\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.500     0.667     0.571         6\n",
      "         Low      0.250     0.200     0.222         5\n",
      "      Medium      0.556     0.500     0.526        10\n",
      "\n",
      "    accuracy                          0.476        21\n",
      "   macro avg      0.435     0.456     0.440        21\n",
      "weighted avg      0.467     0.476     0.467        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       4    2       0\n",
       "Low        0    1       4\n",
       "Medium     4    1       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: ExtraTrees — Accuracy: 0.476  Macro F1: 0.440\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.500     0.667     0.571         6\n",
      "         Low      0.250     0.200     0.222         5\n",
      "      Medium      0.556     0.500     0.526        10\n",
      "\n",
      "    accuracy                          0.476        21\n",
      "   macro avg      0.435     0.456     0.440        21\n",
      "weighted avg      0.467     0.476     0.467        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       4    2       0\n",
       "Low        0    1       4\n",
       "Medium     4    1       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: GradientBoosting — Accuracy: 0.476  Macro F1: 0.440\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.500     0.667     0.571         6\n",
      "         Low      0.250     0.200     0.222         5\n",
      "      Medium      0.556     0.500     0.526        10\n",
      "\n",
      "    accuracy                          0.476        21\n",
      "   macro avg      0.435     0.456     0.440        21\n",
      "weighted avg      0.467     0.476     0.467        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       4    2       0\n",
       "Low        0    1       4\n",
       "Medium     4    1       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: AdaBoost — Accuracy: 0.429  Macro F1: 0.410\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.333     0.333     0.333         6\n",
      "         Low      0.250     0.400     0.308         5\n",
      "      Medium      0.714     0.500     0.588        10\n",
      "\n",
      "    accuracy                          0.429        21\n",
      "   macro avg      0.433     0.411     0.410        21\n",
      "weighted avg      0.495     0.429     0.449        21\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High  Low  Medium\n",
       "High       2    4       0\n",
       "Low        1    2       2\n",
       "Medium     3    2       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Accuracy  Macro F1\n",
       "0  LogisticRegression     0.762     0.760\n",
       "1                 SVC     0.667     0.638\n",
       "2        RandomForest     0.476     0.440\n",
       "3          ExtraTrees     0.476     0.440\n",
       "4    GradientBoosting     0.476     0.440\n",
       "5            AdaBoost     0.429     0.410\n",
       "6                 KNN     0.476     0.357\n",
       "7          GaussianNB     0.429     0.278"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Safe, consistent model comparison ----\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Ensure features variable matches your df column names\n",
    "features = ['wordcount', 'sentence_count', 'flesch_reading_ease', 'avg_sentence_length', 'lexical_diversity']\n",
    "X = df[features].fillna(0)\n",
    "y = le.transform(df['quality_label'])  # use your existing LabelEncoder\n",
    "\n",
    "# Train/test are already defined (X_train, X_test, y_train, y_test)\n",
    "# But ensure they come from X,y above. If not, recreate:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# Define models as pipelines so every model accepts raw X\n",
    "models = {\n",
    "    \"LogisticRegression\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))]),\n",
    "    \"SVC\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42))]),\n",
    "    \"KNN\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier(n_neighbors=5))]),\n",
    "    \"GaussianNB\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", GaussianNB())]),\n",
    "    \"RandomForest\": Pipeline([(\"clf\", RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42))]),\n",
    "    \"ExtraTrees\": Pipeline([(\"clf\", ExtraTreesClassifier(n_estimators=200, class_weight='balanced', random_state=42))]),\n",
    "    \"GradientBoosting\": Pipeline([(\"clf\", GradientBoostingClassifier(n_estimators=200, random_state=42))]),\n",
    "    \"AdaBoost\": Pipeline([(\"clf\", AdaBoostClassifier(n_estimators=200, random_state=42))])\n",
    "}\n",
    "\n",
    "results = []\n",
    "fitted_models = {}\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)                      # ALWAYS pass raw X_train\n",
    "    y_pred = pipe.predict(X_test)                   # ALWAYS pass raw X_test\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average='macro')\n",
    "    results.append({\"Model\": name, \"Accuracy\": round(acc,3), \"Macro F1\": round(f1m,3)})\n",
    "    fitted_models[name] = pipe\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Model: {name} — Accuracy: {acc:.3f}  Macro F1: {f1m:.3f}\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_, digits=3))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    display(cm_df)\n",
    "\n",
    "summary_df = pd.DataFrame(results).sort_values(by=\"Macro F1\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nSummary:\")\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39c746-f9e9-4ae7-98ff-13b3e032d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-5:Real-Time Analysis Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "61e2d2ba-1842-4556-b1d0-1fbf24b8441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared df_for_similarity with columns: ['url', 'title', 'bodytext', 'wordcount', 'clean_bodytext', 'embedding', 'is_thin', 'flesch_reading_ease', 'sentence_count', 'avg_sentence_length', 'lexical_diversity', 'quality_label', 'text']\n",
      "Non-empty text rows: 69 of 69\n",
      "DEBUG analyze_url: ['No matches above threshold=0.35. Returning top 5 matches with scores.']\n",
      "{\n",
      "  \"url\": \"https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\",\n",
      "  \"error\": null,\n",
      "  \"title\": \"Natural Language Processing is Fun! | by Adam Geitgey | Medium\",\n",
      "  \"word_count\": 4096,\n",
      "  \"sentence_count\": 191,\n",
      "  \"avg_sentence_length\": 21.445,\n",
      "  \"readability\": 57.29,\n",
      "  \"lexical_diversity\": 0.313,\n",
      "  \"quality_label\": \"High\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": [\n",
      "    {\n",
      "      \"url\": \"https://medium.com/@amitvsolutions/machine-learning-101-the-complete-beginners-guide-to-machine-learning-686a30cbcf6b\",\n",
      "      \"similarity\": 0.2035,\n",
      "      \"index\": 30\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.akkio.com/beginners-guide-to-machine-learning\",\n",
      "      \"similarity\": 0.1733,\n",
      "      \"index\": 29\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://aws.amazon.com/what-is/deep-learning/\",\n",
      "      \"similarity\": 0.133,\n",
      "      \"index\": 35\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://developers.google.com/search/docs/fundamentals/seo-starter-guide\",\n",
      "      \"similarity\": 0.1209,\n",
      "      \"index\": 36\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.geeksforgeeks.org/data-science/data-science-with-python-tutorial/\",\n",
      "      \"similarity\": 0.1119,\n",
      "      \"index\": 32\n",
      "    }\n",
      "  ],\n",
      "  \"similar_to_above_threshold\": [],\n",
      "  \"notes\": [\n",
      "    \"No matches above threshold=0.35. Returning top 5 matches with scores.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Interactive runners ready.\n",
      "- To use console prompt: run_interactive()\n",
      "- To use notebook UI with upload: run_widget_with_upload()\n"
     ]
    }
   ],
   "source": [
    "# Improved interactive + upload runner cell (safe, with fallbacks)\n",
    "import json, io, re, traceback, sys\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Ensure pandas, etc are imported (assumes analyze_url already defined and uses pandas)\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    print(\"pandas not available - please install it (pip install pandas).\")\n",
    "\n",
    "# ---- URL parsing helpers (same as you used) ----\n",
    "def _parse_urls_from_text(text):\n",
    "    lines = [ln.strip() for ln in re.split(r'[\\r\\n,]+', text) if ln.strip()]\n",
    "    return lines\n",
    "\n",
    "def _parse_urls_from_csv_bytes(bytes_content):\n",
    "    try:\n",
    "        s = io.StringIO(bytes_content.decode('utf-8', errors='replace'))\n",
    "        df_in = pd.read_csv(s)\n",
    "        url_cols = [c for c in df_in.columns if c.lower() == 'url' or 'url' in c.lower()]\n",
    "        if url_cols:\n",
    "            return df_in[url_cols[0]].dropna().astype(str).tolist()\n",
    "        if df_in.shape[1] == 1:\n",
    "            return df_in.iloc[:,0].dropna().astype(str).tolist()\n",
    "        return []\n",
    "    except Exception:\n",
    "        try:\n",
    "            text = bytes_content.decode('utf-8', errors='replace')\n",
    "            return _parse_urls_from_text(text)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# ---- Console interactive runner ----\n",
    "def run_interactive(default_df_name=\"df\"):\n",
    "    \"\"\"\n",
    "    Console prompt: accepts a single URL or local file path containing multiple URLs.\n",
    "    Uses analyze_url(url, df=...) from the notebook if available.\n",
    "    \"\"\"\n",
    "    df_to_use = globals().get(default_df_name, None)\n",
    "    try:\n",
    "        print(\"Enter a single URL, or a path to a local file containing URLs (newline-separated or CSV).\")\n",
    "        user_in = input(\"URL or file path (leave empty to cancel): \").strip()\n",
    "        if not user_in:\n",
    "            print(\"No input provided. Exiting.\")\n",
    "            return None\n",
    "\n",
    "        # Gather URLs\n",
    "        if user_in.startswith(\"http://\") or user_in.startswith(\"https://\"):\n",
    "            urls = [user_in]\n",
    "        else:\n",
    "            try:\n",
    "                with open(user_in, \"rb\") as f:\n",
    "                    content = f.read()\n",
    "                urls = _parse_urls_from_csv_bytes(content)\n",
    "                if not urls:\n",
    "                    urls = _parse_urls_from_text(content.decode('utf-8', errors='replace'))\n",
    "                if not urls:\n",
    "                    print(\"No URLs found in file; treating input as literal URL.\")\n",
    "                    urls = [user_in]\n",
    "            except FileNotFoundError:\n",
    "                urls = [user_in]\n",
    "\n",
    "        print(f\"Analyzing {len(urls)} URL(s)...\")\n",
    "        results = []\n",
    "        for u in urls:\n",
    "            print(f\"\\n--- {u} ---\")\n",
    "            try:\n",
    "                res = analyze_url(u, df=df_to_use)\n",
    "                print(json.dumps(res, indent=2))\n",
    "                results.append(res)\n",
    "            except Exception as e:\n",
    "                print(\"Error analyzing:\", e)\n",
    "                traceback.print_exc()\n",
    "                results.append({\"url\": u, \"error\": str(e)})\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"run_interactive failed:\", e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ---- Try to enable widget UI; if not available, don't crash ----\n",
    "WIDGETS_AVAILABLE = True\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "except Exception as e:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"ipywidgets not available. To use the notebook GUI, install ipywidgets and enable it.\")\n",
    "    print(\"You can install with: pip install ipywidgets\")\n",
    "    print(\"Then restart the kernel and run this cell again. Falling back to console run_interactive().\")\n",
    "\n",
    "# ---- Widget-based UI with upload support (only if ipywidgets available) ----\n",
    "if WIDGETS_AVAILABLE:\n",
    "    def run_widget_with_upload(default_df_name=\"df\"):\n",
    "        \"\"\"\n",
    "        Jupyter widget UI: text box + Analyze button and FileUpload for bulk URLs.\n",
    "        Uses analyze_url(url, df=...) from the notebook if available.\n",
    "        \"\"\"\n",
    "        txt = widgets.Text(value='', placeholder='Enter URL here', description='URL:', layout=widgets.Layout(width='70%'))\n",
    "        analyze_btn = widgets.Button(description='Analyze URL', button_style='primary')\n",
    "        upload = widgets.FileUpload(accept='.txt,.csv', multiple=False, description='Upload file')\n",
    "        analyze_file_btn = widgets.Button(description='Analyze Uploaded', button_style='success')\n",
    "        out = widgets.Output(layout={'border': '1px solid #ddd', 'max_height': '400px', 'overflow': 'auto'})\n",
    "\n",
    "        def analyze_single_url(url):\n",
    "            df_to_use = globals().get(default_df_name, None)\n",
    "            try:\n",
    "                res = analyze_url(url, df=df_to_use)\n",
    "                return res\n",
    "            except Exception as e:\n",
    "                return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "        def on_click_analyze(b):\n",
    "            with out:\n",
    "                clear_output()\n",
    "                url = txt.value.strip()\n",
    "                if not url:\n",
    "                    print(\"Please enter a URL in the text box.\")\n",
    "                    return\n",
    "                print(f\"Analyzing: {url}\")\n",
    "                try:\n",
    "                    res = analyze_single_url(url)\n",
    "                    print(json.dumps(res, indent=2))\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\", e)\n",
    "                    traceback.print_exc()\n",
    "\n",
    "        def on_click_file(b):\n",
    "            with out:\n",
    "                clear_output()\n",
    "                if not upload.value:\n",
    "                    print(\"Please upload a .txt or .csv file containing URLs.\")\n",
    "                    return\n",
    "                key = list(upload.value.keys())[0]\n",
    "                content = upload.value[key]['content']\n",
    "                fname = key\n",
    "                print(f\"Processing uploaded file: {fname}\")\n",
    "                urls = []\n",
    "                if fname.lower().endswith('.csv'):\n",
    "                    urls = _parse_urls_from_csv_bytes(content)\n",
    "                else:\n",
    "                    try:\n",
    "                        text = content.decode('utf-8', errors='replace')\n",
    "                        urls = _parse_urls_from_text(text)\n",
    "                    except Exception:\n",
    "                        urls = _parse_urls_from_csv_bytes(content)\n",
    "                if not urls:\n",
    "                    print(\"No URLs found in uploaded file.\")\n",
    "                    return\n",
    "                print(f\"Found {len(urls)} URL(s). Running analysis...\")\n",
    "                df_to_use = globals().get(default_df_name, None)\n",
    "                for u in urls:\n",
    "                    print(f\"\\n--- {u} ---\")\n",
    "                    try:\n",
    "                        res = analyze_url(u, df=df_to_use)\n",
    "                        print(json.dumps(res, indent=2))\n",
    "                    except Exception as e:\n",
    "                        print(\"Error analyzing:\", e)\n",
    "                        traceback.print_exc()\n",
    "                print(\"\\nDone.\")\n",
    "\n",
    "        analyze_btn.on_click(on_click_analyze)\n",
    "        analyze_file_btn.on_click(on_click_file)\n",
    "\n",
    "        ui = widgets.VBox([\n",
    "            widgets.HBox([txt, analyze_btn]),\n",
    "            widgets.HBox([upload, analyze_file_btn]),\n",
    "            out\n",
    "        ])\n",
    "        display(ui)\n",
    "else:\n",
    "    # Provide a stub that tells the user to install ipywidgets\n",
    "    def run_widget_with_upload(*args, **kwargs):\n",
    "        print(\"ipywidgets is not installed or available in this environment.\")\n",
    "        print(\"Install it with: pip install ipywidgets and restart the kernel, then re-run this cell.\")\n",
    "        return None\n",
    "# Option 1: create df_text that analyze_url expects, then run analysis\n",
    "# Assumes your original dataframe is called `df`\n",
    "\n",
    "# 1. Create a 'text' column from best available field\n",
    "df_for_similarity = df.copy()\n",
    "# prefer 'clean_bodytext' if present, else 'bodytext', else 'body_text' etc.\n",
    "if 'clean_bodytext' in df_for_similarity.columns:\n",
    "    df_for_similarity['text'] = df_for_similarity['clean_bodytext'].astype(str)\n",
    "elif 'bodytext' in df_for_similarity.columns:\n",
    "    df_for_similarity['text'] = df_for_similarity['bodytext'].astype(str)\n",
    "elif 'body_text' in df_for_similarity.columns:\n",
    "    df_for_similarity['text'] = df_for_similarity['body_text'].astype(str)\n",
    "else:\n",
    "    # fallback: try to assemble from title + body if available\n",
    "    if 'title' in df_for_similarity.columns and 'bodytext' in df_for_similarity.columns:\n",
    "        df_for_similarity['text'] = (df_for_similarity['title'].fillna('') + '. ' + df_for_similarity['bodytext'].fillna('')).astype(str)\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not find a suitable text column in df; expected 'clean_bodytext' or 'bodytext' or 'title'+'bodytext'.\")\n",
    "\n",
    "# 2. Quick sanity checks\n",
    "print(\"Prepared df_for_similarity with columns:\", df_for_similarity.columns.tolist())\n",
    "print(\"Non-empty text rows:\", df_for_similarity['text'].str.strip().astype(bool).sum(), \"of\", len(df_for_similarity))\n",
    "\n",
    "# 3. Run analyze_url with this df (lower threshold so you see candidates)\n",
    "res = analyze_url(\n",
    "    \"https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\",\n",
    "    df=df_for_similarity,\n",
    "    top_k=5,\n",
    "    similarity_threshold=0.35,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "import json\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "def analyze_url(url, df=None, top_k=3, similarity_threshold=0.6, timeout=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Scrape URL, extract features, assign quality label, and find similar docs in df.\n",
    "    - df: optional pandas.DataFrame with a 'text' column (and optional 'url' column).\n",
    "    - top_k: maximum number of similar matches to return\n",
    "    - similarity_threshold: minimum cosine similarity to report a match (0-1)\n",
    "    - verbose: if True prints debug info to stdout\n",
    "    Returns a dict suitable for json.dumps(...)\n",
    "    \"\"\"\n",
    "    result = {\"url\": url, \"error\": None}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; AnalyzerBot/1.0)\"}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        html = resp.text\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Failed to fetch URL: {e}\"\n",
    "        return result\n",
    "\n",
    "    text = _extract_main_text(html, url)\n",
    "    title = None\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        if soup.title and soup.title.string:\n",
    "            title = soup.title.string.strip()\n",
    "    except Exception:\n",
    "        title = None\n",
    "\n",
    "    wc = _word_count(text)\n",
    "    scount = _sentence_count(text)\n",
    "    try:\n",
    "        readability = float(textstat.flesch_reading_ease(text)) if text else 0.0\n",
    "    except Exception:\n",
    "        readability = 0.0\n",
    "\n",
    "    avg_sent_len = wc / scount if scount else 0.0\n",
    "    lex_div = _lexical_diversity(text)\n",
    "\n",
    "    quality_label = _assign_quality_label(wc, readability)\n",
    "    is_thin = (wc < 300) or (quality_label == \"Low\")\n",
    "\n",
    "    result.update({\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"word_count\": int(wc),\n",
    "        \"sentence_count\": int(scount),\n",
    "        \"avg_sentence_length\": float(round(avg_sent_len, 3)),\n",
    "        \"readability\": float(round(readability, 3)),\n",
    "        \"lexical_diversity\": float(round(lex_div, 3)),\n",
    "        \"quality_label\": quality_label,\n",
    "        \"is_thin\": bool(is_thin),\n",
    "        \"similar_to\": [],\n",
    "        \"similar_to_above_threshold\": []\n",
    "    })\n",
    "\n",
    "    # -------- Improved similarity logic ----------\n",
    "    try:\n",
    "        notes = []\n",
    "        if isinstance(df, pd.DataFrame) and 'text' in df.columns and len(df) > 0:\n",
    "            corpus_series = df['text'].fillna(\"\").astype(str)\n",
    "            if corpus_series.str.strip().eq(\"\").all():\n",
    "                notes.append(\"Corpus 'text' column is present but all values are empty.\")\n",
    "            corpus_texts = corpus_series.tolist()\n",
    "\n",
    "            if 'url' in df.columns:\n",
    "                urls = df['url'].fillna(\"\").astype(str).tolist()\n",
    "            else:\n",
    "                urls = [f\"<row_{i}>\" for i in range(len(corpus_texts))]\n",
    "\n",
    "            nonempty_idx = [i for i, t in enumerate(corpus_texts) if t.strip() != \"\"]\n",
    "            if not nonempty_idx:\n",
    "                notes.append(\"No non-empty corpus documents to compare against.\")\n",
    "            else:\n",
    "                corpus_texts_nonempty = [corpus_texts[i] for i in nonempty_idx]\n",
    "                urls_nonempty = [urls[i] for i in nonempty_idx]\n",
    "\n",
    "                vect = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "                X_corpus = vect.fit_transform(corpus_texts_nonempty)\n",
    "                X_doc = vect.transform([text])\n",
    "\n",
    "                if X_doc.nnz == 0:\n",
    "                    notes.append(\"Document vector is empty after TF-IDF (too short or no overlapping vocabulary). Similarity will be zero.\")\n",
    "                    sims = np.zeros(len(urls_nonempty))\n",
    "                else:\n",
    "                    sims = cosine_similarity(X_doc, X_corpus).flatten()\n",
    "\n",
    "                matches_all = []\n",
    "                for idx_local, sim in enumerate(sims):\n",
    "                    orig_idx = nonempty_idx[idx_local]\n",
    "                    matches_all.append({\n",
    "                        \"url\": urls_nonempty[idx_local],\n",
    "                        \"similarity\": float(round(float(sim), 4)),\n",
    "                        \"index\": int(orig_idx)\n",
    "                    })\n",
    "\n",
    "                matches_all_sorted = sorted(matches_all, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "                result[\"similar_to\"] = matches_all_sorted[:top_k]\n",
    "                result[\"similar_to_above_threshold\"] = [m for m in matches_all_sorted if m[\"similarity\"] >= similarity_threshold][:top_k]\n",
    "\n",
    "                if not result[\"similar_to_above_threshold\"]:\n",
    "                    notes.append(f\"No matches above threshold={similarity_threshold}. Returning top {top_k} matches with scores.\")\n",
    "        else:\n",
    "            notes.append(\"No DataFrame with a 'text' column was provided for similarity checks.\")\n",
    "\n",
    "        if notes:\n",
    "            result.setdefault(\"notes\", []).extend(notes)\n",
    "    except Exception as e:\n",
    "        result.setdefault(\"notes\", [])\n",
    "        result[\"notes\"].append(f\"Similarity check failed: {e}\")\n",
    "        result[\"similar_to\"] = []\n",
    "        result[\"similar_to_above_threshold\"] = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DEBUG analyze_url:\", result.get(\"notes\", \"no notes\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---- Quick note: user might need to restart kernel after installing ipywidgets ----\n",
    "print(\"\\nInteractive runners ready.\")\n",
    "print(\"- To use console prompt: run_interactive()\")\n",
    "if WIDGETS_AVAILABLE:\n",
    "    print(\"- To use notebook UI with upload: run_widget_with_upload()\")\n",
    "else:\n",
    "    print(\"- Notebook UI not available. Install ipywidgets and restart kernel to enable run_widget_with_upload().\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "7e78228a-87ba-4e4c-92c0-33be22a37ac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[448], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# local utils\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_html, extract_main_text\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_features_from_text\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QualityScorer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# streamlit_app/app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# local utils\n",
    "from utils.parser import fetch_html, extract_main_text\n",
    "from utils.features import extract_features_from_text\n",
    "from utils.scorer import QualityScorer\n",
    "\n",
    "APP_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = APP_DIR.parent\n",
    "\n",
    "st.set_page_config(page_title=\"SEO Content Detector\", layout=\"centered\")\n",
    "st.title(\"SEO Content Quality & Duplicate Detector\")\n",
    "\n",
    "# Sidebar controls\n",
    "st.sidebar.header(\"Corpus & Settings\")\n",
    "use_default = st.sidebar.checkbox(\"Use provided corpus (data/extracted_content.csv)\", value=True)\n",
    "uploaded = st.sidebar.file_uploader(\"Or upload corpus CSV (.csv with 'url' and 'text' columns)\", type=[\"csv\"])\n",
    "similarity_threshold = st.sidebar.slider(\"Similarity threshold\", 0.0, 1.0, 0.35, 0.05)\n",
    "top_k = st.sidebar.slider(\"Top K matches\", 1, 10, 5)\n",
    "\n",
    "# Load or prepare corpus\n",
    "corpus_df = None\n",
    "if use_default:\n",
    "    for cand in [\"data/extracted_content.csv\", \"data/data.csv\"]:\n",
    "        p = ROOT_DIR / cand\n",
    "        if p.exists():\n",
    "            try:\n",
    "                corpus_df = pd.read_csv(p)\n",
    "                st.sidebar.write(f\"Loaded corpus: {cand} ({len(corpus_df)} rows)\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                st.sidebar.warning(f\"Failed to load {cand}: {e}\")\n",
    "if uploaded is not None:\n",
    "    try:\n",
    "        uploaded_bytes = uploaded.read()\n",
    "        tmp = pd.read_csv(io.BytesIO(uploaded_bytes))\n",
    "        corpus_df = tmp\n",
    "        st.sidebar.write(f\"Uploaded corpus ({len(corpus_df)} rows)\")\n",
    "    except Exception as e:\n",
    "        st.sidebar.error(f\"Failed to read uploaded file: {e}\")\n",
    "\n",
    "# Normalize corpus text column name if present\n",
    "if isinstance(corpus_df, pd.DataFrame):\n",
    "    if 'text' not in corpus_df.columns:\n",
    "        if 'clean_bodytext' in corpus_df.columns:\n",
    "            corpus_df['text'] = corpus_df['clean_bodytext'].astype(str)\n",
    "        elif 'bodytext' in corpus_df.columns:\n",
    "            corpus_df['text'] = corpus_df['bodytext'].astype(str)\n",
    "        elif 'body' in corpus_df.columns:\n",
    "            corpus_df['text'] = corpus_df['body'].astype(str)\n",
    "        else:\n",
    "            st.sidebar.warning(\"Corpus has no 'text' column — similarity disabled.\")\n",
    "            corpus_df = None\n",
    "\n",
    "# Initialize scorer (loads quality_model.pkl if present; uses rule fallback otherwise)\n",
    "scorer = QualityScorer(model_path=APP_DIR / \"models\" / \"quality_model.pkl\")\n",
    "\n",
    "# UI: URL input\n",
    "st.subheader(\"Analyze a URL\")\n",
    "url = st.text_input(\"Enter article URL\", \"\")\n",
    "analyze = st.button(\"Analyze\")\n",
    "\n",
    "def build_tfidf(corpus_texts):\n",
    "    vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2), stop_words='english')\n",
    "    X = vect.fit_transform(corpus_texts)\n",
    "    return vect, X\n",
    "\n",
    "if analyze:\n",
    "    if not url.strip():\n",
    "        st.error(\"Please enter a URL.\")\n",
    "    else:\n",
    "        with st.spinner(\"Fetching and analyzing...\"):\n",
    "            try:\n",
    "                html = fetch_html(url)\n",
    "                text = extract_main_text(html)\n",
    "                features = extract_features_from_text(text)\n",
    "                label, probs = scorer.predict_label(features)  # label str, probs dict\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to analyze URL: {e}\")\n",
    "                st.stop()\n",
    "\n",
    "        # Show results\n",
    "        st.markdown(\"### Quality Summary\")\n",
    "        st.write({\n",
    "            \"URL\": url,\n",
    "            \"Title\": features.get(\"title\", \"\"),\n",
    "            \"Word count\": features[\"wordcount\"],\n",
    "            \"Readability (Flesch)\": features[\"flesch_reading_ease\"],\n",
    "            \"Avg sentence len\": round(features[\"avg_sentence_length\"], 2),\n",
    "            \"Lexical diversity\": round(features[\"lexical_diversity\"], 3),\n",
    "            \"Quality label\": label\n",
    "        })\n",
    "\n",
    "        # Show text sample\n",
    "        st.markdown(\"### Extracted Text (first 800 chars)\")\n",
    "        st.text(text[:800] + (\"...\" if len(text) > 800 else \"\"))\n",
    "\n",
    "        # Similarity checks (if corpus available)\n",
    "        if isinstance(corpus_df, pd.DataFrame) and len(corpus_df) > 0:\n",
    "            st.markdown(\"### Similarity / Duplicate Check\")\n",
    "            corpus_texts = corpus_df['text'].fillna(\"\").astype(str).tolist()\n",
    "            corpus_urls = corpus_df['url'].fillna(\"\").astype(str).tolist() if 'url' in corpus_df.columns else [f\"row_{i}\" for i in range(len(corpus_texts))]\n",
    "\n",
    "            # build TF-IDF\n",
    "            try:\n",
    "                vect, X_corpus = build_tfidf(corpus_texts)\n",
    "                X_q = vect.transform([text])\n",
    "                if X_q.nnz == 0:\n",
    "                    st.info(\"Query produced empty TF-IDF vector (too short or vocabulary mismatch). Try lower threshold or use embeddings.\")\n",
    "                else:\n",
    "                    sims = cosine_similarity(X_q, X_corpus).flatten()\n",
    "                    idx = np.argsort(-sims)[:top_k]\n",
    "                    matches = [{\"url\": corpus_urls[i], \"similarity\": float(round(float(sims[i]),4)), \"index\": int(i)} for i in idx]\n",
    "                    st.table(pd.DataFrame(matches))\n",
    "                    above = [m for m in matches if m[\"similarity\"] >= similarity_threshold]\n",
    "                    if above:\n",
    "                        st.success(f\"{len(above)} matches above threshold ({similarity_threshold})\")\n",
    "                        st.table(pd.DataFrame(above))\n",
    "                    else:\n",
    "                        st.info(f\"No matches exceeded the threshold {similarity_threshold}. Showing top {len(matches)} candidates.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Similarity check failed: {e}\")\n",
    "        else:\n",
    "            st.info(\"No corpus loaded for similarity checks (enable sidebar option or upload).\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"Notes: uses TF-IDF similarity and simple readability & length rules. For semantic similarity use precomputed embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126fb72-ab06-4c64-a46f-e0f8400ce9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
